Carlos Martin Sanchez <cmartin@opennebula.org>	 Wed, Feb 8, 2012 at 12:59 PM
To: Paul Grandperrin <paul.grandperrin@alterway.fr>
Cc: users@lists.opennebula.org
Hi Paul,

This is a very interesting feature. You should open a new ecosystem project [1] as soon as your code is usable, so others can test it. If you would like to see your code merged upstream once it gets to a mature enough state, make sure that whoever has to give the thumbs up in your company is aware of the License Agreement [2].

And now some quick comments to your questions:


On Mon, Feb 6, 2012 at 12:19 PM, Paul Grandperrin <paul.grandperrin@alterway.fr> wrote:
1. About VM memory scaling: Currently, AFAIK the vm.memory is used when deploying a VM to set it's initial memory and then is regularly updated via hypervisor polling.
    ATM, i'm also using this attribute to change memory size. I think it's really not the best way thing to do. I'd like to separate theses different things in separate variables.
    For exemple:
       -memory: the same as of now.
       -memory_target: the target amount of memory when scaling memory.

    I could also use VM history but I'm not very familiar with this class.


Each history entry represents a host change, so new ones are created only when the VM is deployed, migrated, or stopped + resumed. That's not the best place to log the scaling changes.

About storing the target amount of memory: VM::memory is the used memory, as reported by the polling. The memory definition, set by the user and used to create the deployment file, is taken from the MEMORY attribute of VM::obj_template. I think you should overwrite that attribute to store the target memory.

Before doing this scaling operation, you should check that the host has enough free memory. After the operation, the host share should be updated, take a look at Host::host_share, Host::add_capacity and Host::del_capacity. If you don't update the host share memory, when the VM is shutdown it will leave the host with a negative memory value.

2. When scalling the number of VCPUs, should we also scale the VM's cpu share? If so, how to implement it?

I'm not sure about the desirable behaviour. Maybe this should be decided by the user? If you are going to modify the CPU, and not only the VCPU, all the above comments about the MEMORY apply.

3. In the case of a scalling failure (memory or vcpu), what should we do?
   -Consider the VM failed and not usable anymore? (I think it's way too strict)
   -Consider the VM still ACTIVE. However, how to inform the user about the failure (something else than writing in logs).
    And then what should we do?
       -immediatly throw a monitor request to update to the correct value?
       -Consider the worst case: if scaling down the memory => consider the old value/ if scaling up the memory, consider the new value
       -Other ideas?

I've seen you are creating new LCM states. This can be very tricky, maybe you should just apply the action without moving from the RUNNING state, like the reboot action. And, if you are creating new states, at least try to keep it simple and merge those two new ones into just one. SCALING, or even a more generic... HOTPLUG?

I would always return to the RUNNING state, updating MEMORY and CPU (and Host:: host_share) in case of success.
The user will see that the operation finished, and will see if it succeeded taking a look at the VM template. You can also include an error message in the template if the operation failed.

If the scaling command returns success/failure immediately, I would not force a poll. As I said, the poll updates the used memory, not the amount set for the VM.


Regards... and good luck!

[1] http://opennebula.org/community:ecosystem
[2] http://opennebula.org/community:contribute

--
Carlos Martin, MSc
Project Engineer
OpenNebula - The Open Source Toolkit for Data Center Virtualization
www.OpenNebula.org | cmartin@opennebula.org | @OpenNebula